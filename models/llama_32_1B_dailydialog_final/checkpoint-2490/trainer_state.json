{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 2490,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008033741715203857,
      "grad_norm": 1.132008671760559,
      "learning_rate": 0.0001996251673360107,
      "loss": 3.4651,
      "step": 10
    },
    {
      "epoch": 0.016067483430407713,
      "grad_norm": 0.47358328104019165,
      "learning_rate": 0.00019908969210174032,
      "loss": 0.6456,
      "step": 20
    },
    {
      "epoch": 0.02410122514561157,
      "grad_norm": 0.27274540066719055,
      "learning_rate": 0.0001985542168674699,
      "loss": 0.5358,
      "step": 30
    },
    {
      "epoch": 0.03213496686081543,
      "grad_norm": 0.2697281539440155,
      "learning_rate": 0.00019801874163319946,
      "loss": 0.4868,
      "step": 40
    },
    {
      "epoch": 0.04016870857601928,
      "grad_norm": 0.3254106938838959,
      "learning_rate": 0.00019748326639892907,
      "loss": 0.5358,
      "step": 50
    },
    {
      "epoch": 0.04820245029122314,
      "grad_norm": 0.3368348479270935,
      "learning_rate": 0.00019694779116465866,
      "loss": 0.5444,
      "step": 60
    },
    {
      "epoch": 0.056236192006426995,
      "grad_norm": 0.2133895754814148,
      "learning_rate": 0.00019641231593038822,
      "loss": 0.4039,
      "step": 70
    },
    {
      "epoch": 0.06426993372163085,
      "grad_norm": 0.3219776153564453,
      "learning_rate": 0.00019587684069611783,
      "loss": 0.4511,
      "step": 80
    },
    {
      "epoch": 0.0723036754368347,
      "grad_norm": 0.30520257353782654,
      "learning_rate": 0.00019534136546184742,
      "loss": 0.4859,
      "step": 90
    },
    {
      "epoch": 0.08033741715203856,
      "grad_norm": 0.28878071904182434,
      "learning_rate": 0.00019480589022757697,
      "loss": 0.4267,
      "step": 100
    },
    {
      "epoch": 0.08837115886724242,
      "grad_norm": 0.2603961229324341,
      "learning_rate": 0.00019427041499330656,
      "loss": 0.4352,
      "step": 110
    },
    {
      "epoch": 0.09640490058244627,
      "grad_norm": 0.2261233627796173,
      "learning_rate": 0.00019373493975903617,
      "loss": 0.4654,
      "step": 120
    },
    {
      "epoch": 0.10443864229765012,
      "grad_norm": 0.2439500093460083,
      "learning_rate": 0.00019319946452476573,
      "loss": 0.4454,
      "step": 130
    },
    {
      "epoch": 0.11247238401285399,
      "grad_norm": 0.24835407733917236,
      "learning_rate": 0.00019266398929049532,
      "loss": 0.4423,
      "step": 140
    },
    {
      "epoch": 0.12050612572805784,
      "grad_norm": 0.2820044159889221,
      "learning_rate": 0.00019212851405622493,
      "loss": 0.5221,
      "step": 150
    },
    {
      "epoch": 0.1285398674432617,
      "grad_norm": 0.2305702269077301,
      "learning_rate": 0.0001915930388219545,
      "loss": 0.4302,
      "step": 160
    },
    {
      "epoch": 0.13657360915846556,
      "grad_norm": 0.21414883434772491,
      "learning_rate": 0.00019105756358768407,
      "loss": 0.4498,
      "step": 170
    },
    {
      "epoch": 0.1446073508736694,
      "grad_norm": 0.22551341354846954,
      "learning_rate": 0.00019052208835341369,
      "loss": 0.4543,
      "step": 180
    },
    {
      "epoch": 0.15264109258887326,
      "grad_norm": 0.22646573185920715,
      "learning_rate": 0.00018998661311914324,
      "loss": 0.4838,
      "step": 190
    },
    {
      "epoch": 0.1606748343040771,
      "grad_norm": 0.22690428793430328,
      "learning_rate": 0.00018945113788487283,
      "loss": 0.4376,
      "step": 200
    },
    {
      "epoch": 0.168708576019281,
      "grad_norm": 0.2163514792919159,
      "learning_rate": 0.00018891566265060242,
      "loss": 0.4391,
      "step": 210
    },
    {
      "epoch": 0.17674231773448484,
      "grad_norm": 0.23566676676273346,
      "learning_rate": 0.000188380187416332,
      "loss": 0.4775,
      "step": 220
    },
    {
      "epoch": 0.1847760594496887,
      "grad_norm": 0.21516543626785278,
      "learning_rate": 0.0001878447121820616,
      "loss": 0.4381,
      "step": 230
    },
    {
      "epoch": 0.19280980116489255,
      "grad_norm": 0.24172808229923248,
      "learning_rate": 0.00018730923694779117,
      "loss": 0.4331,
      "step": 240
    },
    {
      "epoch": 0.2008435428800964,
      "grad_norm": 0.26499494910240173,
      "learning_rate": 0.00018677376171352076,
      "loss": 0.4032,
      "step": 250
    },
    {
      "epoch": 0.20887728459530025,
      "grad_norm": 0.23649801313877106,
      "learning_rate": 0.00018623828647925034,
      "loss": 0.4158,
      "step": 260
    },
    {
      "epoch": 0.21691102631050413,
      "grad_norm": 0.20637226104736328,
      "learning_rate": 0.00018570281124497993,
      "loss": 0.4153,
      "step": 270
    },
    {
      "epoch": 0.22494476802570798,
      "grad_norm": 0.22066718339920044,
      "learning_rate": 0.00018516733601070952,
      "loss": 0.4103,
      "step": 280
    },
    {
      "epoch": 0.23297850974091183,
      "grad_norm": 0.1991472840309143,
      "learning_rate": 0.0001846318607764391,
      "loss": 0.4392,
      "step": 290
    },
    {
      "epoch": 0.24101225145611568,
      "grad_norm": 0.2619296908378601,
      "learning_rate": 0.0001840963855421687,
      "loss": 0.4674,
      "step": 300
    },
    {
      "epoch": 0.24904599317131954,
      "grad_norm": 0.2240721583366394,
      "learning_rate": 0.00018356091030789827,
      "loss": 0.4833,
      "step": 310
    },
    {
      "epoch": 0.2570797348865234,
      "grad_norm": 0.20903994143009186,
      "learning_rate": 0.00018302543507362786,
      "loss": 0.4732,
      "step": 320
    },
    {
      "epoch": 0.26511347660172724,
      "grad_norm": 0.16380032896995544,
      "learning_rate": 0.00018248995983935744,
      "loss": 0.4103,
      "step": 330
    },
    {
      "epoch": 0.2731472183169311,
      "grad_norm": 0.20191846787929535,
      "learning_rate": 0.00018195448460508703,
      "loss": 0.4657,
      "step": 340
    },
    {
      "epoch": 0.28118096003213494,
      "grad_norm": 0.2295970320701599,
      "learning_rate": 0.00018141900937081661,
      "loss": 0.4196,
      "step": 350
    },
    {
      "epoch": 0.2892147017473388,
      "grad_norm": 0.20007199048995972,
      "learning_rate": 0.0001808835341365462,
      "loss": 0.4883,
      "step": 360
    },
    {
      "epoch": 0.2972484434625427,
      "grad_norm": 0.17337796092033386,
      "learning_rate": 0.00018034805890227579,
      "loss": 0.4095,
      "step": 370
    },
    {
      "epoch": 0.3052821851777465,
      "grad_norm": 0.20329415798187256,
      "learning_rate": 0.00017981258366800537,
      "loss": 0.4449,
      "step": 380
    },
    {
      "epoch": 0.3133159268929504,
      "grad_norm": 0.16563549637794495,
      "learning_rate": 0.00017927710843373496,
      "loss": 0.4468,
      "step": 390
    },
    {
      "epoch": 0.3213496686081542,
      "grad_norm": 0.23716925084590912,
      "learning_rate": 0.00017874163319946454,
      "loss": 0.4099,
      "step": 400
    },
    {
      "epoch": 0.3293834103233581,
      "grad_norm": 0.20661552250385284,
      "learning_rate": 0.0001782061579651941,
      "loss": 0.409,
      "step": 410
    },
    {
      "epoch": 0.337417152038562,
      "grad_norm": 0.1957511156797409,
      "learning_rate": 0.00017767068273092371,
      "loss": 0.3842,
      "step": 420
    },
    {
      "epoch": 0.3454508937537658,
      "grad_norm": 0.1871669888496399,
      "learning_rate": 0.0001771352074966533,
      "loss": 0.456,
      "step": 430
    },
    {
      "epoch": 0.3534846354689697,
      "grad_norm": 0.1884123980998993,
      "learning_rate": 0.00017659973226238286,
      "loss": 0.4141,
      "step": 440
    },
    {
      "epoch": 0.3615183771841735,
      "grad_norm": 0.20582698285579681,
      "learning_rate": 0.00017606425702811247,
      "loss": 0.4629,
      "step": 450
    },
    {
      "epoch": 0.3695521188993774,
      "grad_norm": 0.24805234372615814,
      "learning_rate": 0.00017552878179384206,
      "loss": 0.4226,
      "step": 460
    },
    {
      "epoch": 0.3775858606145812,
      "grad_norm": 0.20405986905097961,
      "learning_rate": 0.00017499330655957162,
      "loss": 0.3968,
      "step": 470
    },
    {
      "epoch": 0.3856196023297851,
      "grad_norm": 0.20246562361717224,
      "learning_rate": 0.00017445783132530123,
      "loss": 0.4368,
      "step": 480
    },
    {
      "epoch": 0.393653344044989,
      "grad_norm": 0.2127695381641388,
      "learning_rate": 0.00017392235609103081,
      "loss": 0.4763,
      "step": 490
    },
    {
      "epoch": 0.4016870857601928,
      "grad_norm": 0.17338618636131287,
      "learning_rate": 0.00017338688085676037,
      "loss": 0.507,
      "step": 500
    },
    {
      "epoch": 0.4097208274753967,
      "grad_norm": 0.22318929433822632,
      "learning_rate": 0.00017285140562248996,
      "loss": 0.4136,
      "step": 510
    },
    {
      "epoch": 0.4177545691906005,
      "grad_norm": 0.21907584369182587,
      "learning_rate": 0.00017231593038821957,
      "loss": 0.4341,
      "step": 520
    },
    {
      "epoch": 0.4257883109058044,
      "grad_norm": 0.21428196132183075,
      "learning_rate": 0.00017178045515394913,
      "loss": 0.4283,
      "step": 530
    },
    {
      "epoch": 0.43382205262100826,
      "grad_norm": 0.22335073351860046,
      "learning_rate": 0.00017124497991967871,
      "loss": 0.4515,
      "step": 540
    },
    {
      "epoch": 0.4418557943362121,
      "grad_norm": 0.18083858489990234,
      "learning_rate": 0.00017070950468540833,
      "loss": 0.5201,
      "step": 550
    },
    {
      "epoch": 0.44988953605141596,
      "grad_norm": 0.2371758222579956,
      "learning_rate": 0.00017017402945113789,
      "loss": 0.4027,
      "step": 560
    },
    {
      "epoch": 0.4579232777666198,
      "grad_norm": 0.20005950331687927,
      "learning_rate": 0.00016963855421686747,
      "loss": 0.4316,
      "step": 570
    },
    {
      "epoch": 0.46595701948182366,
      "grad_norm": 0.25928258895874023,
      "learning_rate": 0.00016910307898259708,
      "loss": 0.4413,
      "step": 580
    },
    {
      "epoch": 0.4739907611970275,
      "grad_norm": 0.17014247179031372,
      "learning_rate": 0.00016856760374832664,
      "loss": 0.4388,
      "step": 590
    },
    {
      "epoch": 0.48202450291223137,
      "grad_norm": 0.26768606901168823,
      "learning_rate": 0.00016803212851405623,
      "loss": 0.4119,
      "step": 600
    },
    {
      "epoch": 0.49005824462743525,
      "grad_norm": 0.2324385643005371,
      "learning_rate": 0.00016749665327978581,
      "loss": 0.4288,
      "step": 610
    },
    {
      "epoch": 0.49809198634263907,
      "grad_norm": 0.19439704716205597,
      "learning_rate": 0.0001669611780455154,
      "loss": 0.456,
      "step": 620
    },
    {
      "epoch": 0.506125728057843,
      "grad_norm": 0.24154482781887054,
      "learning_rate": 0.00016642570281124499,
      "loss": 0.3855,
      "step": 630
    },
    {
      "epoch": 0.5141594697730468,
      "grad_norm": 0.21832583844661713,
      "learning_rate": 0.00016589022757697457,
      "loss": 0.4704,
      "step": 640
    },
    {
      "epoch": 0.5221932114882507,
      "grad_norm": 0.23757041990756989,
      "learning_rate": 0.00016535475234270416,
      "loss": 0.4725,
      "step": 650
    },
    {
      "epoch": 0.5302269532034545,
      "grad_norm": 0.21809662878513336,
      "learning_rate": 0.00016481927710843374,
      "loss": 0.4362,
      "step": 660
    },
    {
      "epoch": 0.5382606949186584,
      "grad_norm": 0.2550409138202667,
      "learning_rate": 0.00016428380187416333,
      "loss": 0.4175,
      "step": 670
    },
    {
      "epoch": 0.5462944366338622,
      "grad_norm": 0.1967543065547943,
      "learning_rate": 0.00016374832663989291,
      "loss": 0.4142,
      "step": 680
    },
    {
      "epoch": 0.5543281783490661,
      "grad_norm": 0.22716736793518066,
      "learning_rate": 0.0001632128514056225,
      "loss": 0.3668,
      "step": 690
    },
    {
      "epoch": 0.5623619200642699,
      "grad_norm": 0.21152514219284058,
      "learning_rate": 0.00016267737617135208,
      "loss": 0.4084,
      "step": 700
    },
    {
      "epoch": 0.5703956617794738,
      "grad_norm": 0.17286980152130127,
      "learning_rate": 0.00016214190093708167,
      "loss": 0.4107,
      "step": 710
    },
    {
      "epoch": 0.5784294034946776,
      "grad_norm": 0.34120073914527893,
      "learning_rate": 0.00016160642570281126,
      "loss": 0.4208,
      "step": 720
    },
    {
      "epoch": 0.5864631452098815,
      "grad_norm": 0.23264890909194946,
      "learning_rate": 0.00016107095046854084,
      "loss": 0.4348,
      "step": 730
    },
    {
      "epoch": 0.5944968869250854,
      "grad_norm": 0.22570852935314178,
      "learning_rate": 0.00016053547523427043,
      "loss": 0.3815,
      "step": 740
    },
    {
      "epoch": 0.6025306286402892,
      "grad_norm": 0.21916428208351135,
      "learning_rate": 0.00016,
      "loss": 0.3505,
      "step": 750
    },
    {
      "epoch": 0.610564370355493,
      "grad_norm": 0.2203734964132309,
      "learning_rate": 0.0001594645247657296,
      "loss": 0.3929,
      "step": 760
    },
    {
      "epoch": 0.6185981120706969,
      "grad_norm": 0.17801420390605927,
      "learning_rate": 0.00015892904953145918,
      "loss": 0.3903,
      "step": 770
    },
    {
      "epoch": 0.6266318537859008,
      "grad_norm": 0.19534707069396973,
      "learning_rate": 0.00015839357429718874,
      "loss": 0.3176,
      "step": 780
    },
    {
      "epoch": 0.6346655955011047,
      "grad_norm": 0.2451646625995636,
      "learning_rate": 0.00015785809906291836,
      "loss": 0.4348,
      "step": 790
    },
    {
      "epoch": 0.6426993372163085,
      "grad_norm": 0.22583137452602386,
      "learning_rate": 0.00015732262382864794,
      "loss": 0.3547,
      "step": 800
    },
    {
      "epoch": 0.6507330789315123,
      "grad_norm": 0.1758325695991516,
      "learning_rate": 0.0001567871485943775,
      "loss": 0.338,
      "step": 810
    },
    {
      "epoch": 0.6587668206467162,
      "grad_norm": 0.20006053149700165,
      "learning_rate": 0.0001562516733601071,
      "loss": 0.3391,
      "step": 820
    },
    {
      "epoch": 0.6668005623619201,
      "grad_norm": 0.2508776783943176,
      "learning_rate": 0.0001557161981258367,
      "loss": 0.4166,
      "step": 830
    },
    {
      "epoch": 0.674834304077124,
      "grad_norm": 0.27153199911117554,
      "learning_rate": 0.00015518072289156626,
      "loss": 0.4386,
      "step": 840
    },
    {
      "epoch": 0.6828680457923277,
      "grad_norm": 0.2080046832561493,
      "learning_rate": 0.00015464524765729587,
      "loss": 0.374,
      "step": 850
    },
    {
      "epoch": 0.6909017875075316,
      "grad_norm": 0.17246730625629425,
      "learning_rate": 0.00015410977242302546,
      "loss": 0.3244,
      "step": 860
    },
    {
      "epoch": 0.6989355292227355,
      "grad_norm": 0.2656323313713074,
      "learning_rate": 0.00015357429718875501,
      "loss": 0.4165,
      "step": 870
    },
    {
      "epoch": 0.7069692709379394,
      "grad_norm": 0.2481265813112259,
      "learning_rate": 0.0001530388219544846,
      "loss": 0.3908,
      "step": 880
    },
    {
      "epoch": 0.7150030126531433,
      "grad_norm": 0.22763706743717194,
      "learning_rate": 0.0001525033467202142,
      "loss": 0.3541,
      "step": 890
    },
    {
      "epoch": 0.723036754368347,
      "grad_norm": 0.19910573959350586,
      "learning_rate": 0.00015196787148594377,
      "loss": 0.3795,
      "step": 900
    },
    {
      "epoch": 0.7310704960835509,
      "grad_norm": 0.2121638059616089,
      "learning_rate": 0.00015143239625167336,
      "loss": 0.3562,
      "step": 910
    },
    {
      "epoch": 0.7391042377987548,
      "grad_norm": 0.18302994966506958,
      "learning_rate": 0.00015089692101740297,
      "loss": 0.3453,
      "step": 920
    },
    {
      "epoch": 0.7471379795139587,
      "grad_norm": 0.18843090534210205,
      "learning_rate": 0.00015036144578313253,
      "loss": 0.3854,
      "step": 930
    },
    {
      "epoch": 0.7551717212291624,
      "grad_norm": 0.2186533808708191,
      "learning_rate": 0.0001498259705488621,
      "loss": 0.3722,
      "step": 940
    },
    {
      "epoch": 0.7632054629443663,
      "grad_norm": 0.21920454502105713,
      "learning_rate": 0.00014929049531459173,
      "loss": 0.3959,
      "step": 950
    },
    {
      "epoch": 0.7712392046595702,
      "grad_norm": 0.1929076462984085,
      "learning_rate": 0.00014875502008032128,
      "loss": 0.3837,
      "step": 960
    },
    {
      "epoch": 0.7792729463747741,
      "grad_norm": 0.17810696363449097,
      "learning_rate": 0.00014821954484605087,
      "loss": 0.3452,
      "step": 970
    },
    {
      "epoch": 0.787306688089978,
      "grad_norm": 0.2561005651950836,
      "learning_rate": 0.00014768406961178046,
      "loss": 0.3767,
      "step": 980
    },
    {
      "epoch": 0.7953404298051817,
      "grad_norm": 0.30869072675704956,
      "learning_rate": 0.00014714859437751004,
      "loss": 0.4368,
      "step": 990
    },
    {
      "epoch": 0.8033741715203856,
      "grad_norm": 0.20675168931484222,
      "learning_rate": 0.00014661311914323963,
      "loss": 0.3202,
      "step": 1000
    },
    {
      "epoch": 0.8114079132355895,
      "grad_norm": 0.19101428985595703,
      "learning_rate": 0.0001460776439089692,
      "loss": 0.3756,
      "step": 1010
    },
    {
      "epoch": 0.8194416549507934,
      "grad_norm": 0.2381686121225357,
      "learning_rate": 0.0001455421686746988,
      "loss": 0.3994,
      "step": 1020
    },
    {
      "epoch": 0.8274753966659972,
      "grad_norm": 0.2507588565349579,
      "learning_rate": 0.00014500669344042838,
      "loss": 0.4189,
      "step": 1030
    },
    {
      "epoch": 0.835509138381201,
      "grad_norm": 0.30224180221557617,
      "learning_rate": 0.00014447121820615797,
      "loss": 0.4142,
      "step": 1040
    },
    {
      "epoch": 0.8435428800964049,
      "grad_norm": 0.20850928127765656,
      "learning_rate": 0.00014393574297188756,
      "loss": 0.366,
      "step": 1050
    },
    {
      "epoch": 0.8515766218116088,
      "grad_norm": 0.18256916105747223,
      "learning_rate": 0.00014340026773761714,
      "loss": 0.3213,
      "step": 1060
    },
    {
      "epoch": 0.8596103635268126,
      "grad_norm": 0.18299731612205505,
      "learning_rate": 0.00014286479250334673,
      "loss": 0.3581,
      "step": 1070
    },
    {
      "epoch": 0.8676441052420165,
      "grad_norm": 0.15771929919719696,
      "learning_rate": 0.0001423293172690763,
      "loss": 0.393,
      "step": 1080
    },
    {
      "epoch": 0.8756778469572203,
      "grad_norm": 0.21335934102535248,
      "learning_rate": 0.0001417938420348059,
      "loss": 0.401,
      "step": 1090
    },
    {
      "epoch": 0.8837115886724242,
      "grad_norm": 0.23628227412700653,
      "learning_rate": 0.00014125836680053548,
      "loss": 0.402,
      "step": 1100
    },
    {
      "epoch": 0.891745330387628,
      "grad_norm": 0.28371793031692505,
      "learning_rate": 0.00014072289156626507,
      "loss": 0.3997,
      "step": 1110
    },
    {
      "epoch": 0.8997790721028319,
      "grad_norm": 0.2388913333415985,
      "learning_rate": 0.00014018741633199465,
      "loss": 0.3734,
      "step": 1120
    },
    {
      "epoch": 0.9078128138180358,
      "grad_norm": 0.2081468105316162,
      "learning_rate": 0.00013965194109772424,
      "loss": 0.3668,
      "step": 1130
    },
    {
      "epoch": 0.9158465555332396,
      "grad_norm": 0.2579735517501831,
      "learning_rate": 0.00013911646586345383,
      "loss": 0.3427,
      "step": 1140
    },
    {
      "epoch": 0.9238802972484434,
      "grad_norm": 0.19921037554740906,
      "learning_rate": 0.0001385809906291834,
      "loss": 0.3426,
      "step": 1150
    },
    {
      "epoch": 0.9319140389636473,
      "grad_norm": 0.19951798021793365,
      "learning_rate": 0.000138045515394913,
      "loss": 0.3877,
      "step": 1160
    },
    {
      "epoch": 0.9399477806788512,
      "grad_norm": 0.20289741456508636,
      "learning_rate": 0.00013751004016064258,
      "loss": 0.399,
      "step": 1170
    },
    {
      "epoch": 0.947981522394055,
      "grad_norm": 0.2422320395708084,
      "learning_rate": 0.00013697456492637214,
      "loss": 0.4071,
      "step": 1180
    },
    {
      "epoch": 0.9560152641092589,
      "grad_norm": 0.17441679537296295,
      "learning_rate": 0.00013643908969210175,
      "loss": 0.354,
      "step": 1190
    },
    {
      "epoch": 0.9640490058244627,
      "grad_norm": 0.17903202772140503,
      "learning_rate": 0.00013590361445783134,
      "loss": 0.4123,
      "step": 1200
    },
    {
      "epoch": 0.9720827475396666,
      "grad_norm": 0.17970091104507446,
      "learning_rate": 0.0001353681392235609,
      "loss": 0.3548,
      "step": 1210
    },
    {
      "epoch": 0.9801164892548705,
      "grad_norm": 0.34728384017944336,
      "learning_rate": 0.0001348326639892905,
      "loss": 0.4056,
      "step": 1220
    },
    {
      "epoch": 0.9881502309700743,
      "grad_norm": 0.20289669930934906,
      "learning_rate": 0.0001342971887550201,
      "loss": 0.3378,
      "step": 1230
    },
    {
      "epoch": 0.9961839726852781,
      "grad_norm": 0.22148609161376953,
      "learning_rate": 0.00013376171352074965,
      "loss": 0.3503,
      "step": 1240
    },
    {
      "epoch": 1.0040168708576018,
      "grad_norm": 0.21025404334068298,
      "learning_rate": 0.00013322623828647927,
      "loss": 0.4153,
      "step": 1250
    },
    {
      "epoch": 1.0120506125728057,
      "grad_norm": 0.2411973476409912,
      "learning_rate": 0.00013269076305220885,
      "loss": 0.3553,
      "step": 1260
    },
    {
      "epoch": 1.0200843542880096,
      "grad_norm": 0.21799862384796143,
      "learning_rate": 0.0001321552878179384,
      "loss": 0.4613,
      "step": 1270
    },
    {
      "epoch": 1.0281180960032135,
      "grad_norm": 0.246332585811615,
      "learning_rate": 0.000131619812583668,
      "loss": 0.3224,
      "step": 1280
    },
    {
      "epoch": 1.0361518377184173,
      "grad_norm": 0.2858455777168274,
      "learning_rate": 0.0001310843373493976,
      "loss": 0.3304,
      "step": 1290
    },
    {
      "epoch": 1.0441855794336212,
      "grad_norm": 0.24395707249641418,
      "learning_rate": 0.00013054886211512717,
      "loss": 0.3921,
      "step": 1300
    },
    {
      "epoch": 1.052219321148825,
      "grad_norm": 0.23404213786125183,
      "learning_rate": 0.00013001338688085675,
      "loss": 0.3547,
      "step": 1310
    },
    {
      "epoch": 1.060253062864029,
      "grad_norm": 0.2077709138393402,
      "learning_rate": 0.00012947791164658637,
      "loss": 0.3528,
      "step": 1320
    },
    {
      "epoch": 1.0682868045792329,
      "grad_norm": 0.2238084375858307,
      "learning_rate": 0.00012894243641231593,
      "loss": 0.3443,
      "step": 1330
    },
    {
      "epoch": 1.0763205462944367,
      "grad_norm": 0.20972716808319092,
      "learning_rate": 0.0001284069611780455,
      "loss": 0.36,
      "step": 1340
    },
    {
      "epoch": 1.0843542880096404,
      "grad_norm": 0.2123727649450302,
      "learning_rate": 0.00012787148594377512,
      "loss": 0.3648,
      "step": 1350
    },
    {
      "epoch": 1.0923880297248443,
      "grad_norm": 0.17604078352451324,
      "learning_rate": 0.00012733601070950468,
      "loss": 0.3589,
      "step": 1360
    },
    {
      "epoch": 1.1004217714400482,
      "grad_norm": 0.23872920870780945,
      "learning_rate": 0.00012680053547523427,
      "loss": 0.4228,
      "step": 1370
    },
    {
      "epoch": 1.108455513155252,
      "grad_norm": 0.27258363366127014,
      "learning_rate": 0.00012626506024096385,
      "loss": 0.4359,
      "step": 1380
    },
    {
      "epoch": 1.116489254870456,
      "grad_norm": 0.19572122395038605,
      "learning_rate": 0.00012572958500669344,
      "loss": 0.3864,
      "step": 1390
    },
    {
      "epoch": 1.1245229965856598,
      "grad_norm": 0.2099531590938568,
      "learning_rate": 0.00012519410977242303,
      "loss": 0.3648,
      "step": 1400
    },
    {
      "epoch": 1.1325567383008637,
      "grad_norm": 0.20813390612602234,
      "learning_rate": 0.0001246586345381526,
      "loss": 0.4238,
      "step": 1410
    },
    {
      "epoch": 1.1405904800160676,
      "grad_norm": 0.2050723284482956,
      "learning_rate": 0.0001241231593038822,
      "loss": 0.4057,
      "step": 1420
    },
    {
      "epoch": 1.1486242217312714,
      "grad_norm": 0.21433116495609283,
      "learning_rate": 0.00012358768406961178,
      "loss": 0.3699,
      "step": 1430
    },
    {
      "epoch": 1.156657963446475,
      "grad_norm": 0.20369383692741394,
      "learning_rate": 0.00012305220883534137,
      "loss": 0.3622,
      "step": 1440
    },
    {
      "epoch": 1.164691705161679,
      "grad_norm": 0.22480599582195282,
      "learning_rate": 0.00012251673360107095,
      "loss": 0.3709,
      "step": 1450
    },
    {
      "epoch": 1.1727254468768828,
      "grad_norm": 0.24358583986759186,
      "learning_rate": 0.00012198125836680054,
      "loss": 0.3541,
      "step": 1460
    },
    {
      "epoch": 1.1807591885920867,
      "grad_norm": 0.19848763942718506,
      "learning_rate": 0.00012144578313253012,
      "loss": 0.3797,
      "step": 1470
    },
    {
      "epoch": 1.1887929303072906,
      "grad_norm": 0.19707661867141724,
      "learning_rate": 0.0001209103078982597,
      "loss": 0.3842,
      "step": 1480
    },
    {
      "epoch": 1.1968266720224945,
      "grad_norm": 0.21157968044281006,
      "learning_rate": 0.0001203748326639893,
      "loss": 0.3875,
      "step": 1490
    },
    {
      "epoch": 1.2048604137376984,
      "grad_norm": 0.22039122879505157,
      "learning_rate": 0.00011983935742971888,
      "loss": 0.342,
      "step": 1500
    },
    {
      "epoch": 1.2128941554529022,
      "grad_norm": 0.21861785650253296,
      "learning_rate": 0.00011930388219544845,
      "loss": 0.3512,
      "step": 1510
    },
    {
      "epoch": 1.2209278971681061,
      "grad_norm": 0.277175635099411,
      "learning_rate": 0.00011876840696117805,
      "loss": 0.3327,
      "step": 1520
    },
    {
      "epoch": 1.2289616388833098,
      "grad_norm": 0.31667569279670715,
      "learning_rate": 0.00011823293172690764,
      "loss": 0.3824,
      "step": 1530
    },
    {
      "epoch": 1.2369953805985137,
      "grad_norm": 0.28321242332458496,
      "learning_rate": 0.00011769745649263721,
      "loss": 0.3765,
      "step": 1540
    },
    {
      "epoch": 1.2450291223137175,
      "grad_norm": 0.20964811742305756,
      "learning_rate": 0.00011716198125836681,
      "loss": 0.3762,
      "step": 1550
    },
    {
      "epoch": 1.2530628640289214,
      "grad_norm": 0.21235890686511993,
      "learning_rate": 0.0001166265060240964,
      "loss": 0.4174,
      "step": 1560
    },
    {
      "epoch": 1.2610966057441253,
      "grad_norm": 0.18887633085250854,
      "learning_rate": 0.00011609103078982597,
      "loss": 0.3541,
      "step": 1570
    },
    {
      "epoch": 1.2691303474593292,
      "grad_norm": 0.28848013281822205,
      "learning_rate": 0.00011555555555555555,
      "loss": 0.3831,
      "step": 1580
    },
    {
      "epoch": 1.277164089174533,
      "grad_norm": 0.2641180157661438,
      "learning_rate": 0.00011502008032128515,
      "loss": 0.3338,
      "step": 1590
    },
    {
      "epoch": 1.285197830889737,
      "grad_norm": 0.19582973420619965,
      "learning_rate": 0.00011448460508701472,
      "loss": 0.3248,
      "step": 1600
    },
    {
      "epoch": 1.2932315726049408,
      "grad_norm": 0.2550804615020752,
      "learning_rate": 0.00011394912985274431,
      "loss": 0.3856,
      "step": 1610
    },
    {
      "epoch": 1.3012653143201445,
      "grad_norm": 0.19659313559532166,
      "learning_rate": 0.00011341365461847391,
      "loss": 0.38,
      "step": 1620
    },
    {
      "epoch": 1.3092990560353486,
      "grad_norm": 0.2583557963371277,
      "learning_rate": 0.00011287817938420348,
      "loss": 0.3662,
      "step": 1630
    },
    {
      "epoch": 1.3173327977505522,
      "grad_norm": 0.22443416714668274,
      "learning_rate": 0.00011234270414993307,
      "loss": 0.3633,
      "step": 1640
    },
    {
      "epoch": 1.325366539465756,
      "grad_norm": 0.22605152428150177,
      "learning_rate": 0.00011180722891566267,
      "loss": 0.3494,
      "step": 1650
    },
    {
      "epoch": 1.33340028118096,
      "grad_norm": 0.22392763197422028,
      "learning_rate": 0.00011127175368139224,
      "loss": 0.3389,
      "step": 1660
    },
    {
      "epoch": 1.3414340228961639,
      "grad_norm": 0.2192889302968979,
      "learning_rate": 0.00011073627844712182,
      "loss": 0.3472,
      "step": 1670
    },
    {
      "epoch": 1.3494677646113677,
      "grad_norm": 0.20917446911334991,
      "learning_rate": 0.0001102008032128514,
      "loss": 0.3544,
      "step": 1680
    },
    {
      "epoch": 1.3575015063265716,
      "grad_norm": 0.2761863172054291,
      "learning_rate": 0.000109665327978581,
      "loss": 0.3811,
      "step": 1690
    },
    {
      "epoch": 1.3655352480417755,
      "grad_norm": 0.19150669872760773,
      "learning_rate": 0.00010912985274431058,
      "loss": 0.3246,
      "step": 1700
    },
    {
      "epoch": 1.3735689897569794,
      "grad_norm": 0.22120584547519684,
      "learning_rate": 0.00010859437751004015,
      "loss": 0.3329,
      "step": 1710
    },
    {
      "epoch": 1.3816027314721833,
      "grad_norm": 0.23309999704360962,
      "learning_rate": 0.00010805890227576975,
      "loss": 0.3741,
      "step": 1720
    },
    {
      "epoch": 1.389636473187387,
      "grad_norm": 0.2623259127140045,
      "learning_rate": 0.00010752342704149934,
      "loss": 0.4315,
      "step": 1730
    },
    {
      "epoch": 1.3976702149025908,
      "grad_norm": 0.235374316573143,
      "learning_rate": 0.00010698795180722891,
      "loss": 0.3168,
      "step": 1740
    },
    {
      "epoch": 1.4057039566177947,
      "grad_norm": 0.23128081858158112,
      "learning_rate": 0.00010645247657295851,
      "loss": 0.3814,
      "step": 1750
    },
    {
      "epoch": 1.4137376983329986,
      "grad_norm": 0.2060771882534027,
      "learning_rate": 0.0001059170013386881,
      "loss": 0.3694,
      "step": 1760
    },
    {
      "epoch": 1.4217714400482024,
      "grad_norm": 0.2314344197511673,
      "learning_rate": 0.00010538152610441767,
      "loss": 0.3971,
      "step": 1770
    },
    {
      "epoch": 1.4298051817634063,
      "grad_norm": 0.3063316345214844,
      "learning_rate": 0.00010484605087014725,
      "loss": 0.3518,
      "step": 1780
    },
    {
      "epoch": 1.4378389234786102,
      "grad_norm": 0.25503385066986084,
      "learning_rate": 0.00010431057563587685,
      "loss": 0.3999,
      "step": 1790
    },
    {
      "epoch": 1.445872665193814,
      "grad_norm": 0.24630536139011383,
      "learning_rate": 0.00010377510040160642,
      "loss": 0.356,
      "step": 1800
    },
    {
      "epoch": 1.453906406909018,
      "grad_norm": 0.26521536707878113,
      "learning_rate": 0.00010323962516733601,
      "loss": 0.3626,
      "step": 1810
    },
    {
      "epoch": 1.4619401486242216,
      "grad_norm": 0.2676689624786377,
      "learning_rate": 0.00010270414993306561,
      "loss": 0.3422,
      "step": 1820
    },
    {
      "epoch": 1.4699738903394257,
      "grad_norm": 0.24699358642101288,
      "learning_rate": 0.00010216867469879518,
      "loss": 0.3604,
      "step": 1830
    },
    {
      "epoch": 1.4780076320546294,
      "grad_norm": 0.17629535496234894,
      "learning_rate": 0.00010163319946452477,
      "loss": 0.2943,
      "step": 1840
    },
    {
      "epoch": 1.4860413737698333,
      "grad_norm": 0.21683256328105927,
      "learning_rate": 0.00010109772423025437,
      "loss": 0.37,
      "step": 1850
    },
    {
      "epoch": 1.4940751154850371,
      "grad_norm": 0.26659059524536133,
      "learning_rate": 0.00010056224899598394,
      "loss": 0.361,
      "step": 1860
    },
    {
      "epoch": 1.502108857200241,
      "grad_norm": 0.25621628761291504,
      "learning_rate": 0.00010002677376171352,
      "loss": 0.3924,
      "step": 1870
    },
    {
      "epoch": 1.510142598915445,
      "grad_norm": 0.2238258421421051,
      "learning_rate": 9.949129852744311e-05,
      "loss": 0.3396,
      "step": 1880
    },
    {
      "epoch": 1.5181763406306488,
      "grad_norm": 0.23072418570518494,
      "learning_rate": 9.89558232931727e-05,
      "loss": 0.3965,
      "step": 1890
    },
    {
      "epoch": 1.5262100823458526,
      "grad_norm": 0.24812249839305878,
      "learning_rate": 9.842034805890228e-05,
      "loss": 0.3959,
      "step": 1900
    },
    {
      "epoch": 1.5342438240610563,
      "grad_norm": 0.21507136523723602,
      "learning_rate": 9.788487282463187e-05,
      "loss": 0.3535,
      "step": 1910
    },
    {
      "epoch": 1.5422775657762604,
      "grad_norm": 0.2743983268737793,
      "learning_rate": 9.734939759036145e-05,
      "loss": 0.3994,
      "step": 1920
    },
    {
      "epoch": 1.550311307491464,
      "grad_norm": 0.27434444427490234,
      "learning_rate": 9.681392235609104e-05,
      "loss": 0.375,
      "step": 1930
    },
    {
      "epoch": 1.5583450492066682,
      "grad_norm": 0.2638346552848816,
      "learning_rate": 9.627844712182062e-05,
      "loss": 0.3438,
      "step": 1940
    },
    {
      "epoch": 1.5663787909218718,
      "grad_norm": 0.2071731835603714,
      "learning_rate": 9.574297188755021e-05,
      "loss": 0.344,
      "step": 1950
    },
    {
      "epoch": 1.5744125326370757,
      "grad_norm": 0.22280021011829376,
      "learning_rate": 9.52074966532798e-05,
      "loss": 0.3769,
      "step": 1960
    },
    {
      "epoch": 1.5824462743522796,
      "grad_norm": 0.22103025019168854,
      "learning_rate": 9.467202141900937e-05,
      "loss": 0.406,
      "step": 1970
    },
    {
      "epoch": 1.5904800160674835,
      "grad_norm": 0.22083590924739838,
      "learning_rate": 9.413654618473896e-05,
      "loss": 0.356,
      "step": 1980
    },
    {
      "epoch": 1.5985137577826873,
      "grad_norm": 0.23231397569179535,
      "learning_rate": 9.360107095046855e-05,
      "loss": 0.4066,
      "step": 1990
    },
    {
      "epoch": 1.606547499497891,
      "grad_norm": 0.25895750522613525,
      "learning_rate": 9.306559571619812e-05,
      "loss": 0.3718,
      "step": 2000
    },
    {
      "epoch": 1.614581241213095,
      "grad_norm": 0.2096288651227951,
      "learning_rate": 9.253012048192772e-05,
      "loss": 0.4003,
      "step": 2010
    },
    {
      "epoch": 1.6226149829282988,
      "grad_norm": 0.238925039768219,
      "learning_rate": 9.19946452476573e-05,
      "loss": 0.3564,
      "step": 2020
    },
    {
      "epoch": 1.6306487246435029,
      "grad_norm": 0.30842646956443787,
      "learning_rate": 9.145917001338688e-05,
      "loss": 0.3862,
      "step": 2030
    },
    {
      "epoch": 1.6386824663587065,
      "grad_norm": 0.19030800461769104,
      "learning_rate": 9.092369477911648e-05,
      "loss": 0.337,
      "step": 2040
    },
    {
      "epoch": 1.6467162080739104,
      "grad_norm": 0.3022247850894928,
      "learning_rate": 9.038821954484605e-05,
      "loss": 0.3682,
      "step": 2050
    },
    {
      "epoch": 1.6547499497891143,
      "grad_norm": 0.26470932364463806,
      "learning_rate": 8.985274431057564e-05,
      "loss": 0.3741,
      "step": 2060
    },
    {
      "epoch": 1.6627836915043182,
      "grad_norm": 0.22066441178321838,
      "learning_rate": 8.931726907630522e-05,
      "loss": 0.3595,
      "step": 2070
    },
    {
      "epoch": 1.670817433219522,
      "grad_norm": 0.21666333079338074,
      "learning_rate": 8.878179384203481e-05,
      "loss": 0.3428,
      "step": 2080
    },
    {
      "epoch": 1.6788511749347257,
      "grad_norm": 0.18693774938583374,
      "learning_rate": 8.824631860776439e-05,
      "loss": 0.3571,
      "step": 2090
    },
    {
      "epoch": 1.6868849166499298,
      "grad_norm": 0.27557119727134705,
      "learning_rate": 8.771084337349398e-05,
      "loss": 0.3137,
      "step": 2100
    },
    {
      "epoch": 1.6949186583651334,
      "grad_norm": 0.24219098687171936,
      "learning_rate": 8.717536813922356e-05,
      "loss": 0.3755,
      "step": 2110
    },
    {
      "epoch": 1.7029524000803375,
      "grad_norm": 0.22057904303073883,
      "learning_rate": 8.663989290495315e-05,
      "loss": 0.3169,
      "step": 2120
    },
    {
      "epoch": 1.7109861417955412,
      "grad_norm": 0.3494814336299896,
      "learning_rate": 8.610441767068274e-05,
      "loss": 0.4038,
      "step": 2130
    },
    {
      "epoch": 1.719019883510745,
      "grad_norm": 0.19113487005233765,
      "learning_rate": 8.556894243641232e-05,
      "loss": 0.3087,
      "step": 2140
    },
    {
      "epoch": 1.727053625225949,
      "grad_norm": 0.2614189684391022,
      "learning_rate": 8.503346720214191e-05,
      "loss": 0.355,
      "step": 2150
    },
    {
      "epoch": 1.7350873669411528,
      "grad_norm": 0.19660761952400208,
      "learning_rate": 8.449799196787149e-05,
      "loss": 0.3892,
      "step": 2160
    },
    {
      "epoch": 1.7431211086563567,
      "grad_norm": 0.21846309304237366,
      "learning_rate": 8.396251673360106e-05,
      "loss": 0.337,
      "step": 2170
    },
    {
      "epoch": 1.7511548503715606,
      "grad_norm": 0.2698502242565155,
      "learning_rate": 8.342704149933066e-05,
      "loss": 0.3334,
      "step": 2180
    },
    {
      "epoch": 1.7591885920867645,
      "grad_norm": 0.28796523809432983,
      "learning_rate": 8.289156626506025e-05,
      "loss": 0.4511,
      "step": 2190
    },
    {
      "epoch": 1.7672223338019681,
      "grad_norm": 0.3226747214794159,
      "learning_rate": 8.235609103078982e-05,
      "loss": 0.3582,
      "step": 2200
    },
    {
      "epoch": 1.7752560755171722,
      "grad_norm": 0.24429668486118317,
      "learning_rate": 8.182061579651942e-05,
      "loss": 0.3969,
      "step": 2210
    },
    {
      "epoch": 1.783289817232376,
      "grad_norm": 0.21187929809093475,
      "learning_rate": 8.128514056224899e-05,
      "loss": 0.3585,
      "step": 2220
    },
    {
      "epoch": 1.79132355894758,
      "grad_norm": 0.2742381989955902,
      "learning_rate": 8.074966532797858e-05,
      "loss": 0.3725,
      "step": 2230
    },
    {
      "epoch": 1.7993573006627837,
      "grad_norm": 0.19131051003932953,
      "learning_rate": 8.021419009370818e-05,
      "loss": 0.322,
      "step": 2240
    },
    {
      "epoch": 1.8073910423779875,
      "grad_norm": 0.174168661236763,
      "learning_rate": 7.967871485943775e-05,
      "loss": 0.3103,
      "step": 2250
    },
    {
      "epoch": 1.8154247840931914,
      "grad_norm": 0.21696144342422485,
      "learning_rate": 7.914323962516735e-05,
      "loss": 0.3714,
      "step": 2260
    },
    {
      "epoch": 1.8234585258083953,
      "grad_norm": 0.2576613128185272,
      "learning_rate": 7.860776439089692e-05,
      "loss": 0.3357,
      "step": 2270
    },
    {
      "epoch": 1.8314922675235992,
      "grad_norm": 0.21023418009281158,
      "learning_rate": 7.80722891566265e-05,
      "loss": 0.3237,
      "step": 2280
    },
    {
      "epoch": 1.8395260092388028,
      "grad_norm": 0.22956319153308868,
      "learning_rate": 7.75368139223561e-05,
      "loss": 0.3979,
      "step": 2290
    },
    {
      "epoch": 1.847559750954007,
      "grad_norm": 0.2887933850288391,
      "learning_rate": 7.700133868808568e-05,
      "loss": 0.4164,
      "step": 2300
    },
    {
      "epoch": 1.8555934926692106,
      "grad_norm": 0.28453823924064636,
      "learning_rate": 7.646586345381526e-05,
      "loss": 0.405,
      "step": 2310
    },
    {
      "epoch": 1.8636272343844147,
      "grad_norm": 0.20546390116214752,
      "learning_rate": 7.593038821954485e-05,
      "loss": 0.3605,
      "step": 2320
    },
    {
      "epoch": 1.8716609760996183,
      "grad_norm": 0.23203156888484955,
      "learning_rate": 7.539491298527443e-05,
      "loss": 0.3813,
      "step": 2330
    },
    {
      "epoch": 1.8796947178148222,
      "grad_norm": 0.22656196355819702,
      "learning_rate": 7.485943775100402e-05,
      "loss": 0.3669,
      "step": 2340
    },
    {
      "epoch": 1.887728459530026,
      "grad_norm": 0.21479547023773193,
      "learning_rate": 7.43239625167336e-05,
      "loss": 0.3794,
      "step": 2350
    },
    {
      "epoch": 1.89576220124523,
      "grad_norm": 0.2951812744140625,
      "learning_rate": 7.378848728246319e-05,
      "loss": 0.3353,
      "step": 2360
    },
    {
      "epoch": 1.9037959429604339,
      "grad_norm": 0.25887784361839294,
      "learning_rate": 7.325301204819278e-05,
      "loss": 0.3819,
      "step": 2370
    },
    {
      "epoch": 1.9118296846756375,
      "grad_norm": 0.2884334921836853,
      "learning_rate": 7.271753681392236e-05,
      "loss": 0.4064,
      "step": 2380
    },
    {
      "epoch": 1.9198634263908416,
      "grad_norm": 0.23540639877319336,
      "learning_rate": 7.218206157965195e-05,
      "loss": 0.3044,
      "step": 2390
    },
    {
      "epoch": 1.9278971681060453,
      "grad_norm": 0.22783619165420532,
      "learning_rate": 7.164658634538153e-05,
      "loss": 0.3525,
      "step": 2400
    },
    {
      "epoch": 1.9359309098212494,
      "grad_norm": 0.20055140554904938,
      "learning_rate": 7.111111111111112e-05,
      "loss": 0.3365,
      "step": 2410
    },
    {
      "epoch": 1.943964651536453,
      "grad_norm": 0.264608770608902,
      "learning_rate": 7.057563587684069e-05,
      "loss": 0.3423,
      "step": 2420
    },
    {
      "epoch": 1.951998393251657,
      "grad_norm": 0.22867229580879211,
      "learning_rate": 7.004016064257029e-05,
      "loss": 0.3619,
      "step": 2430
    },
    {
      "epoch": 1.9600321349668608,
      "grad_norm": 0.29081079363822937,
      "learning_rate": 6.950468540829988e-05,
      "loss": 0.4022,
      "step": 2440
    },
    {
      "epoch": 1.9680658766820647,
      "grad_norm": 0.25540319085121155,
      "learning_rate": 6.896921017402945e-05,
      "loss": 0.3174,
      "step": 2450
    },
    {
      "epoch": 1.9760996183972686,
      "grad_norm": 0.2642761170864105,
      "learning_rate": 6.843373493975905e-05,
      "loss": 0.3948,
      "step": 2460
    },
    {
      "epoch": 1.9841333601124724,
      "grad_norm": 0.20571234822273254,
      "learning_rate": 6.789825970548862e-05,
      "loss": 0.2872,
      "step": 2470
    },
    {
      "epoch": 1.9921671018276763,
      "grad_norm": 0.2754462957382202,
      "learning_rate": 6.73627844712182e-05,
      "loss": 0.3432,
      "step": 2480
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.36839279532432556,
      "learning_rate": 6.68273092369478e-05,
      "loss": 0.3764,
      "step": 2490
    }
  ],
  "logging_steps": 10,
  "max_steps": 3735,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.974147902681907e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
