{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1245,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008033741715203857,
      "grad_norm": 1.132008671760559,
      "learning_rate": 0.0001996251673360107,
      "loss": 3.4651,
      "step": 10
    },
    {
      "epoch": 0.016067483430407713,
      "grad_norm": 0.47358328104019165,
      "learning_rate": 0.00019908969210174032,
      "loss": 0.6456,
      "step": 20
    },
    {
      "epoch": 0.02410122514561157,
      "grad_norm": 0.27274540066719055,
      "learning_rate": 0.0001985542168674699,
      "loss": 0.5358,
      "step": 30
    },
    {
      "epoch": 0.03213496686081543,
      "grad_norm": 0.2697281539440155,
      "learning_rate": 0.00019801874163319946,
      "loss": 0.4868,
      "step": 40
    },
    {
      "epoch": 0.04016870857601928,
      "grad_norm": 0.3254106938838959,
      "learning_rate": 0.00019748326639892907,
      "loss": 0.5358,
      "step": 50
    },
    {
      "epoch": 0.04820245029122314,
      "grad_norm": 0.3368348479270935,
      "learning_rate": 0.00019694779116465866,
      "loss": 0.5444,
      "step": 60
    },
    {
      "epoch": 0.056236192006426995,
      "grad_norm": 0.2133895754814148,
      "learning_rate": 0.00019641231593038822,
      "loss": 0.4039,
      "step": 70
    },
    {
      "epoch": 0.06426993372163085,
      "grad_norm": 0.3219776153564453,
      "learning_rate": 0.00019587684069611783,
      "loss": 0.4511,
      "step": 80
    },
    {
      "epoch": 0.0723036754368347,
      "grad_norm": 0.30520257353782654,
      "learning_rate": 0.00019534136546184742,
      "loss": 0.4859,
      "step": 90
    },
    {
      "epoch": 0.08033741715203856,
      "grad_norm": 0.28878071904182434,
      "learning_rate": 0.00019480589022757697,
      "loss": 0.4267,
      "step": 100
    },
    {
      "epoch": 0.08837115886724242,
      "grad_norm": 0.2603961229324341,
      "learning_rate": 0.00019427041499330656,
      "loss": 0.4352,
      "step": 110
    },
    {
      "epoch": 0.09640490058244627,
      "grad_norm": 0.2261233627796173,
      "learning_rate": 0.00019373493975903617,
      "loss": 0.4654,
      "step": 120
    },
    {
      "epoch": 0.10443864229765012,
      "grad_norm": 0.2439500093460083,
      "learning_rate": 0.00019319946452476573,
      "loss": 0.4454,
      "step": 130
    },
    {
      "epoch": 0.11247238401285399,
      "grad_norm": 0.24835407733917236,
      "learning_rate": 0.00019266398929049532,
      "loss": 0.4423,
      "step": 140
    },
    {
      "epoch": 0.12050612572805784,
      "grad_norm": 0.2820044159889221,
      "learning_rate": 0.00019212851405622493,
      "loss": 0.5221,
      "step": 150
    },
    {
      "epoch": 0.1285398674432617,
      "grad_norm": 0.2305702269077301,
      "learning_rate": 0.0001915930388219545,
      "loss": 0.4302,
      "step": 160
    },
    {
      "epoch": 0.13657360915846556,
      "grad_norm": 0.21414883434772491,
      "learning_rate": 0.00019105756358768407,
      "loss": 0.4498,
      "step": 170
    },
    {
      "epoch": 0.1446073508736694,
      "grad_norm": 0.22551341354846954,
      "learning_rate": 0.00019052208835341369,
      "loss": 0.4543,
      "step": 180
    },
    {
      "epoch": 0.15264109258887326,
      "grad_norm": 0.22646573185920715,
      "learning_rate": 0.00018998661311914324,
      "loss": 0.4838,
      "step": 190
    },
    {
      "epoch": 0.1606748343040771,
      "grad_norm": 0.22690428793430328,
      "learning_rate": 0.00018945113788487283,
      "loss": 0.4376,
      "step": 200
    },
    {
      "epoch": 0.168708576019281,
      "grad_norm": 0.2163514792919159,
      "learning_rate": 0.00018891566265060242,
      "loss": 0.4391,
      "step": 210
    },
    {
      "epoch": 0.17674231773448484,
      "grad_norm": 0.23566676676273346,
      "learning_rate": 0.000188380187416332,
      "loss": 0.4775,
      "step": 220
    },
    {
      "epoch": 0.1847760594496887,
      "grad_norm": 0.21516543626785278,
      "learning_rate": 0.0001878447121820616,
      "loss": 0.4381,
      "step": 230
    },
    {
      "epoch": 0.19280980116489255,
      "grad_norm": 0.24172808229923248,
      "learning_rate": 0.00018730923694779117,
      "loss": 0.4331,
      "step": 240
    },
    {
      "epoch": 0.2008435428800964,
      "grad_norm": 0.26499494910240173,
      "learning_rate": 0.00018677376171352076,
      "loss": 0.4032,
      "step": 250
    },
    {
      "epoch": 0.20887728459530025,
      "grad_norm": 0.23649801313877106,
      "learning_rate": 0.00018623828647925034,
      "loss": 0.4158,
      "step": 260
    },
    {
      "epoch": 0.21691102631050413,
      "grad_norm": 0.20637226104736328,
      "learning_rate": 0.00018570281124497993,
      "loss": 0.4153,
      "step": 270
    },
    {
      "epoch": 0.22494476802570798,
      "grad_norm": 0.22066718339920044,
      "learning_rate": 0.00018516733601070952,
      "loss": 0.4103,
      "step": 280
    },
    {
      "epoch": 0.23297850974091183,
      "grad_norm": 0.1991472840309143,
      "learning_rate": 0.0001846318607764391,
      "loss": 0.4392,
      "step": 290
    },
    {
      "epoch": 0.24101225145611568,
      "grad_norm": 0.2619296908378601,
      "learning_rate": 0.0001840963855421687,
      "loss": 0.4674,
      "step": 300
    },
    {
      "epoch": 0.24904599317131954,
      "grad_norm": 0.2240721583366394,
      "learning_rate": 0.00018356091030789827,
      "loss": 0.4833,
      "step": 310
    },
    {
      "epoch": 0.2570797348865234,
      "grad_norm": 0.20903994143009186,
      "learning_rate": 0.00018302543507362786,
      "loss": 0.4732,
      "step": 320
    },
    {
      "epoch": 0.26511347660172724,
      "grad_norm": 0.16380032896995544,
      "learning_rate": 0.00018248995983935744,
      "loss": 0.4103,
      "step": 330
    },
    {
      "epoch": 0.2731472183169311,
      "grad_norm": 0.20191846787929535,
      "learning_rate": 0.00018195448460508703,
      "loss": 0.4657,
      "step": 340
    },
    {
      "epoch": 0.28118096003213494,
      "grad_norm": 0.2295970320701599,
      "learning_rate": 0.00018141900937081661,
      "loss": 0.4196,
      "step": 350
    },
    {
      "epoch": 0.2892147017473388,
      "grad_norm": 0.20007199048995972,
      "learning_rate": 0.0001808835341365462,
      "loss": 0.4883,
      "step": 360
    },
    {
      "epoch": 0.2972484434625427,
      "grad_norm": 0.17337796092033386,
      "learning_rate": 0.00018034805890227579,
      "loss": 0.4095,
      "step": 370
    },
    {
      "epoch": 0.3052821851777465,
      "grad_norm": 0.20329415798187256,
      "learning_rate": 0.00017981258366800537,
      "loss": 0.4449,
      "step": 380
    },
    {
      "epoch": 0.3133159268929504,
      "grad_norm": 0.16563549637794495,
      "learning_rate": 0.00017927710843373496,
      "loss": 0.4468,
      "step": 390
    },
    {
      "epoch": 0.3213496686081542,
      "grad_norm": 0.23716925084590912,
      "learning_rate": 0.00017874163319946454,
      "loss": 0.4099,
      "step": 400
    },
    {
      "epoch": 0.3293834103233581,
      "grad_norm": 0.20661552250385284,
      "learning_rate": 0.0001782061579651941,
      "loss": 0.409,
      "step": 410
    },
    {
      "epoch": 0.337417152038562,
      "grad_norm": 0.1957511156797409,
      "learning_rate": 0.00017767068273092371,
      "loss": 0.3842,
      "step": 420
    },
    {
      "epoch": 0.3454508937537658,
      "grad_norm": 0.1871669888496399,
      "learning_rate": 0.0001771352074966533,
      "loss": 0.456,
      "step": 430
    },
    {
      "epoch": 0.3534846354689697,
      "grad_norm": 0.1884123980998993,
      "learning_rate": 0.00017659973226238286,
      "loss": 0.4141,
      "step": 440
    },
    {
      "epoch": 0.3615183771841735,
      "grad_norm": 0.20582698285579681,
      "learning_rate": 0.00017606425702811247,
      "loss": 0.4629,
      "step": 450
    },
    {
      "epoch": 0.3695521188993774,
      "grad_norm": 0.24805234372615814,
      "learning_rate": 0.00017552878179384206,
      "loss": 0.4226,
      "step": 460
    },
    {
      "epoch": 0.3775858606145812,
      "grad_norm": 0.20405986905097961,
      "learning_rate": 0.00017499330655957162,
      "loss": 0.3968,
      "step": 470
    },
    {
      "epoch": 0.3856196023297851,
      "grad_norm": 0.20246562361717224,
      "learning_rate": 0.00017445783132530123,
      "loss": 0.4368,
      "step": 480
    },
    {
      "epoch": 0.393653344044989,
      "grad_norm": 0.2127695381641388,
      "learning_rate": 0.00017392235609103081,
      "loss": 0.4763,
      "step": 490
    },
    {
      "epoch": 0.4016870857601928,
      "grad_norm": 0.17338618636131287,
      "learning_rate": 0.00017338688085676037,
      "loss": 0.507,
      "step": 500
    },
    {
      "epoch": 0.4097208274753967,
      "grad_norm": 0.22318929433822632,
      "learning_rate": 0.00017285140562248996,
      "loss": 0.4136,
      "step": 510
    },
    {
      "epoch": 0.4177545691906005,
      "grad_norm": 0.21907584369182587,
      "learning_rate": 0.00017231593038821957,
      "loss": 0.4341,
      "step": 520
    },
    {
      "epoch": 0.4257883109058044,
      "grad_norm": 0.21428196132183075,
      "learning_rate": 0.00017178045515394913,
      "loss": 0.4283,
      "step": 530
    },
    {
      "epoch": 0.43382205262100826,
      "grad_norm": 0.22335073351860046,
      "learning_rate": 0.00017124497991967871,
      "loss": 0.4515,
      "step": 540
    },
    {
      "epoch": 0.4418557943362121,
      "grad_norm": 0.18083858489990234,
      "learning_rate": 0.00017070950468540833,
      "loss": 0.5201,
      "step": 550
    },
    {
      "epoch": 0.44988953605141596,
      "grad_norm": 0.2371758222579956,
      "learning_rate": 0.00017017402945113789,
      "loss": 0.4027,
      "step": 560
    },
    {
      "epoch": 0.4579232777666198,
      "grad_norm": 0.20005950331687927,
      "learning_rate": 0.00016963855421686747,
      "loss": 0.4316,
      "step": 570
    },
    {
      "epoch": 0.46595701948182366,
      "grad_norm": 0.25928258895874023,
      "learning_rate": 0.00016910307898259708,
      "loss": 0.4413,
      "step": 580
    },
    {
      "epoch": 0.4739907611970275,
      "grad_norm": 0.17014247179031372,
      "learning_rate": 0.00016856760374832664,
      "loss": 0.4388,
      "step": 590
    },
    {
      "epoch": 0.48202450291223137,
      "grad_norm": 0.26768606901168823,
      "learning_rate": 0.00016803212851405623,
      "loss": 0.4119,
      "step": 600
    },
    {
      "epoch": 0.49005824462743525,
      "grad_norm": 0.2324385643005371,
      "learning_rate": 0.00016749665327978581,
      "loss": 0.4288,
      "step": 610
    },
    {
      "epoch": 0.49809198634263907,
      "grad_norm": 0.19439704716205597,
      "learning_rate": 0.0001669611780455154,
      "loss": 0.456,
      "step": 620
    },
    {
      "epoch": 0.506125728057843,
      "grad_norm": 0.24154482781887054,
      "learning_rate": 0.00016642570281124499,
      "loss": 0.3855,
      "step": 630
    },
    {
      "epoch": 0.5141594697730468,
      "grad_norm": 0.21832583844661713,
      "learning_rate": 0.00016589022757697457,
      "loss": 0.4704,
      "step": 640
    },
    {
      "epoch": 0.5221932114882507,
      "grad_norm": 0.23757041990756989,
      "learning_rate": 0.00016535475234270416,
      "loss": 0.4725,
      "step": 650
    },
    {
      "epoch": 0.5302269532034545,
      "grad_norm": 0.21809662878513336,
      "learning_rate": 0.00016481927710843374,
      "loss": 0.4362,
      "step": 660
    },
    {
      "epoch": 0.5382606949186584,
      "grad_norm": 0.2550409138202667,
      "learning_rate": 0.00016428380187416333,
      "loss": 0.4175,
      "step": 670
    },
    {
      "epoch": 0.5462944366338622,
      "grad_norm": 0.1967543065547943,
      "learning_rate": 0.00016374832663989291,
      "loss": 0.4142,
      "step": 680
    },
    {
      "epoch": 0.5543281783490661,
      "grad_norm": 0.22716736793518066,
      "learning_rate": 0.0001632128514056225,
      "loss": 0.3668,
      "step": 690
    },
    {
      "epoch": 0.5623619200642699,
      "grad_norm": 0.21152514219284058,
      "learning_rate": 0.00016267737617135208,
      "loss": 0.4084,
      "step": 700
    },
    {
      "epoch": 0.5703956617794738,
      "grad_norm": 0.17286980152130127,
      "learning_rate": 0.00016214190093708167,
      "loss": 0.4107,
      "step": 710
    },
    {
      "epoch": 0.5784294034946776,
      "grad_norm": 0.34120073914527893,
      "learning_rate": 0.00016160642570281126,
      "loss": 0.4208,
      "step": 720
    },
    {
      "epoch": 0.5864631452098815,
      "grad_norm": 0.23264890909194946,
      "learning_rate": 0.00016107095046854084,
      "loss": 0.4348,
      "step": 730
    },
    {
      "epoch": 0.5944968869250854,
      "grad_norm": 0.22570852935314178,
      "learning_rate": 0.00016053547523427043,
      "loss": 0.3815,
      "step": 740
    },
    {
      "epoch": 0.6025306286402892,
      "grad_norm": 0.21916428208351135,
      "learning_rate": 0.00016,
      "loss": 0.3505,
      "step": 750
    },
    {
      "epoch": 0.610564370355493,
      "grad_norm": 0.2203734964132309,
      "learning_rate": 0.0001594645247657296,
      "loss": 0.3929,
      "step": 760
    },
    {
      "epoch": 0.6185981120706969,
      "grad_norm": 0.17801420390605927,
      "learning_rate": 0.00015892904953145918,
      "loss": 0.3903,
      "step": 770
    },
    {
      "epoch": 0.6266318537859008,
      "grad_norm": 0.19534707069396973,
      "learning_rate": 0.00015839357429718874,
      "loss": 0.3176,
      "step": 780
    },
    {
      "epoch": 0.6346655955011047,
      "grad_norm": 0.2451646625995636,
      "learning_rate": 0.00015785809906291836,
      "loss": 0.4348,
      "step": 790
    },
    {
      "epoch": 0.6426993372163085,
      "grad_norm": 0.22583137452602386,
      "learning_rate": 0.00015732262382864794,
      "loss": 0.3547,
      "step": 800
    },
    {
      "epoch": 0.6507330789315123,
      "grad_norm": 0.1758325695991516,
      "learning_rate": 0.0001567871485943775,
      "loss": 0.338,
      "step": 810
    },
    {
      "epoch": 0.6587668206467162,
      "grad_norm": 0.20006053149700165,
      "learning_rate": 0.0001562516733601071,
      "loss": 0.3391,
      "step": 820
    },
    {
      "epoch": 0.6668005623619201,
      "grad_norm": 0.2508776783943176,
      "learning_rate": 0.0001557161981258367,
      "loss": 0.4166,
      "step": 830
    },
    {
      "epoch": 0.674834304077124,
      "grad_norm": 0.27153199911117554,
      "learning_rate": 0.00015518072289156626,
      "loss": 0.4386,
      "step": 840
    },
    {
      "epoch": 0.6828680457923277,
      "grad_norm": 0.2080046832561493,
      "learning_rate": 0.00015464524765729587,
      "loss": 0.374,
      "step": 850
    },
    {
      "epoch": 0.6909017875075316,
      "grad_norm": 0.17246730625629425,
      "learning_rate": 0.00015410977242302546,
      "loss": 0.3244,
      "step": 860
    },
    {
      "epoch": 0.6989355292227355,
      "grad_norm": 0.2656323313713074,
      "learning_rate": 0.00015357429718875501,
      "loss": 0.4165,
      "step": 870
    },
    {
      "epoch": 0.7069692709379394,
      "grad_norm": 0.2481265813112259,
      "learning_rate": 0.0001530388219544846,
      "loss": 0.3908,
      "step": 880
    },
    {
      "epoch": 0.7150030126531433,
      "grad_norm": 0.22763706743717194,
      "learning_rate": 0.0001525033467202142,
      "loss": 0.3541,
      "step": 890
    },
    {
      "epoch": 0.723036754368347,
      "grad_norm": 0.19910573959350586,
      "learning_rate": 0.00015196787148594377,
      "loss": 0.3795,
      "step": 900
    },
    {
      "epoch": 0.7310704960835509,
      "grad_norm": 0.2121638059616089,
      "learning_rate": 0.00015143239625167336,
      "loss": 0.3562,
      "step": 910
    },
    {
      "epoch": 0.7391042377987548,
      "grad_norm": 0.18302994966506958,
      "learning_rate": 0.00015089692101740297,
      "loss": 0.3453,
      "step": 920
    },
    {
      "epoch": 0.7471379795139587,
      "grad_norm": 0.18843090534210205,
      "learning_rate": 0.00015036144578313253,
      "loss": 0.3854,
      "step": 930
    },
    {
      "epoch": 0.7551717212291624,
      "grad_norm": 0.2186533808708191,
      "learning_rate": 0.0001498259705488621,
      "loss": 0.3722,
      "step": 940
    },
    {
      "epoch": 0.7632054629443663,
      "grad_norm": 0.21920454502105713,
      "learning_rate": 0.00014929049531459173,
      "loss": 0.3959,
      "step": 950
    },
    {
      "epoch": 0.7712392046595702,
      "grad_norm": 0.1929076462984085,
      "learning_rate": 0.00014875502008032128,
      "loss": 0.3837,
      "step": 960
    },
    {
      "epoch": 0.7792729463747741,
      "grad_norm": 0.17810696363449097,
      "learning_rate": 0.00014821954484605087,
      "loss": 0.3452,
      "step": 970
    },
    {
      "epoch": 0.787306688089978,
      "grad_norm": 0.2561005651950836,
      "learning_rate": 0.00014768406961178046,
      "loss": 0.3767,
      "step": 980
    },
    {
      "epoch": 0.7953404298051817,
      "grad_norm": 0.30869072675704956,
      "learning_rate": 0.00014714859437751004,
      "loss": 0.4368,
      "step": 990
    },
    {
      "epoch": 0.8033741715203856,
      "grad_norm": 0.20675168931484222,
      "learning_rate": 0.00014661311914323963,
      "loss": 0.3202,
      "step": 1000
    },
    {
      "epoch": 0.8114079132355895,
      "grad_norm": 0.19101428985595703,
      "learning_rate": 0.0001460776439089692,
      "loss": 0.3756,
      "step": 1010
    },
    {
      "epoch": 0.8194416549507934,
      "grad_norm": 0.2381686121225357,
      "learning_rate": 0.0001455421686746988,
      "loss": 0.3994,
      "step": 1020
    },
    {
      "epoch": 0.8274753966659972,
      "grad_norm": 0.2507588565349579,
      "learning_rate": 0.00014500669344042838,
      "loss": 0.4189,
      "step": 1030
    },
    {
      "epoch": 0.835509138381201,
      "grad_norm": 0.30224180221557617,
      "learning_rate": 0.00014447121820615797,
      "loss": 0.4142,
      "step": 1040
    },
    {
      "epoch": 0.8435428800964049,
      "grad_norm": 0.20850928127765656,
      "learning_rate": 0.00014393574297188756,
      "loss": 0.366,
      "step": 1050
    },
    {
      "epoch": 0.8515766218116088,
      "grad_norm": 0.18256916105747223,
      "learning_rate": 0.00014340026773761714,
      "loss": 0.3213,
      "step": 1060
    },
    {
      "epoch": 0.8596103635268126,
      "grad_norm": 0.18299731612205505,
      "learning_rate": 0.00014286479250334673,
      "loss": 0.3581,
      "step": 1070
    },
    {
      "epoch": 0.8676441052420165,
      "grad_norm": 0.15771929919719696,
      "learning_rate": 0.0001423293172690763,
      "loss": 0.393,
      "step": 1080
    },
    {
      "epoch": 0.8756778469572203,
      "grad_norm": 0.21335934102535248,
      "learning_rate": 0.0001417938420348059,
      "loss": 0.401,
      "step": 1090
    },
    {
      "epoch": 0.8837115886724242,
      "grad_norm": 0.23628227412700653,
      "learning_rate": 0.00014125836680053548,
      "loss": 0.402,
      "step": 1100
    },
    {
      "epoch": 0.891745330387628,
      "grad_norm": 0.28371793031692505,
      "learning_rate": 0.00014072289156626507,
      "loss": 0.3997,
      "step": 1110
    },
    {
      "epoch": 0.8997790721028319,
      "grad_norm": 0.2388913333415985,
      "learning_rate": 0.00014018741633199465,
      "loss": 0.3734,
      "step": 1120
    },
    {
      "epoch": 0.9078128138180358,
      "grad_norm": 0.2081468105316162,
      "learning_rate": 0.00013965194109772424,
      "loss": 0.3668,
      "step": 1130
    },
    {
      "epoch": 0.9158465555332396,
      "grad_norm": 0.2579735517501831,
      "learning_rate": 0.00013911646586345383,
      "loss": 0.3427,
      "step": 1140
    },
    {
      "epoch": 0.9238802972484434,
      "grad_norm": 0.19921037554740906,
      "learning_rate": 0.0001385809906291834,
      "loss": 0.3426,
      "step": 1150
    },
    {
      "epoch": 0.9319140389636473,
      "grad_norm": 0.19951798021793365,
      "learning_rate": 0.000138045515394913,
      "loss": 0.3877,
      "step": 1160
    },
    {
      "epoch": 0.9399477806788512,
      "grad_norm": 0.20289741456508636,
      "learning_rate": 0.00013751004016064258,
      "loss": 0.399,
      "step": 1170
    },
    {
      "epoch": 0.947981522394055,
      "grad_norm": 0.2422320395708084,
      "learning_rate": 0.00013697456492637214,
      "loss": 0.4071,
      "step": 1180
    },
    {
      "epoch": 0.9560152641092589,
      "grad_norm": 0.17441679537296295,
      "learning_rate": 0.00013643908969210175,
      "loss": 0.354,
      "step": 1190
    },
    {
      "epoch": 0.9640490058244627,
      "grad_norm": 0.17903202772140503,
      "learning_rate": 0.00013590361445783134,
      "loss": 0.4123,
      "step": 1200
    },
    {
      "epoch": 0.9720827475396666,
      "grad_norm": 0.17970091104507446,
      "learning_rate": 0.0001353681392235609,
      "loss": 0.3548,
      "step": 1210
    },
    {
      "epoch": 0.9801164892548705,
      "grad_norm": 0.34728384017944336,
      "learning_rate": 0.0001348326639892905,
      "loss": 0.4056,
      "step": 1220
    },
    {
      "epoch": 0.9881502309700743,
      "grad_norm": 0.20289669930934906,
      "learning_rate": 0.0001342971887550201,
      "loss": 0.3378,
      "step": 1230
    },
    {
      "epoch": 0.9961839726852781,
      "grad_norm": 0.22148609161376953,
      "learning_rate": 0.00013376171352074965,
      "loss": 0.3503,
      "step": 1240
    }
  ],
  "logging_steps": 10,
  "max_steps": 3735,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.9870739513409536e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
