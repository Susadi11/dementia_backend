{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1245,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008034548558802852,
      "grad_norm": 1.0373977422714233,
      "learning_rate": 0.0001997322623828648,
      "loss": 6.9154,
      "step": 10
    },
    {
      "epoch": 0.016069097117605704,
      "grad_norm": 0.30423006415367126,
      "learning_rate": 0.0001991967871485944,
      "loss": 0.7186,
      "step": 20
    },
    {
      "epoch": 0.024103645676408558,
      "grad_norm": 0.18798606097698212,
      "learning_rate": 0.00019866131191432397,
      "loss": 0.5324,
      "step": 30
    },
    {
      "epoch": 0.03213819423521141,
      "grad_norm": 0.14447475969791412,
      "learning_rate": 0.00019812583668005356,
      "loss": 0.4699,
      "step": 40
    },
    {
      "epoch": 0.04017274279401426,
      "grad_norm": 0.23008406162261963,
      "learning_rate": 0.00019759036144578314,
      "loss": 0.5084,
      "step": 50
    },
    {
      "epoch": 0.048207291352817115,
      "grad_norm": 0.2064332216978073,
      "learning_rate": 0.00019705488621151273,
      "loss": 0.5097,
      "step": 60
    },
    {
      "epoch": 0.05624183991161997,
      "grad_norm": 0.1356736272573471,
      "learning_rate": 0.00019651941097724232,
      "loss": 0.3803,
      "step": 70
    },
    {
      "epoch": 0.06427638847042282,
      "grad_norm": 0.17006024718284607,
      "learning_rate": 0.0001959839357429719,
      "loss": 0.4234,
      "step": 80
    },
    {
      "epoch": 0.07231093702922567,
      "grad_norm": 0.22150123119354248,
      "learning_rate": 0.0001954484605087015,
      "loss": 0.4537,
      "step": 90
    },
    {
      "epoch": 0.08034548558802852,
      "grad_norm": 0.20373305678367615,
      "learning_rate": 0.00019491298527443107,
      "loss": 0.4047,
      "step": 100
    },
    {
      "epoch": 0.08838003414683138,
      "grad_norm": 0.15008515119552612,
      "learning_rate": 0.00019437751004016066,
      "loss": 0.4052,
      "step": 110
    },
    {
      "epoch": 0.09641458270563423,
      "grad_norm": 0.14398758113384247,
      "learning_rate": 0.00019384203480589022,
      "loss": 0.4351,
      "step": 120
    },
    {
      "epoch": 0.10444913126443708,
      "grad_norm": 0.16062946617603302,
      "learning_rate": 0.00019330655957161983,
      "loss": 0.4175,
      "step": 130
    },
    {
      "epoch": 0.11248367982323994,
      "grad_norm": 0.1657368242740631,
      "learning_rate": 0.00019277108433734942,
      "loss": 0.4161,
      "step": 140
    },
    {
      "epoch": 0.12051822838204278,
      "grad_norm": 0.20168568193912506,
      "learning_rate": 0.00019223560910307897,
      "loss": 0.4906,
      "step": 150
    },
    {
      "epoch": 0.12855277694084563,
      "grad_norm": 0.1492224782705307,
      "learning_rate": 0.0001917001338688086,
      "loss": 0.4015,
      "step": 160
    },
    {
      "epoch": 0.1365873254996485,
      "grad_norm": 0.15185053646564484,
      "learning_rate": 0.00019116465863453817,
      "loss": 0.4199,
      "step": 170
    },
    {
      "epoch": 0.14462187405845134,
      "grad_norm": 0.13899379968643188,
      "learning_rate": 0.00019062918340026773,
      "loss": 0.4268,
      "step": 180
    },
    {
      "epoch": 0.1526564226172542,
      "grad_norm": 0.1545664668083191,
      "learning_rate": 0.00019009370816599734,
      "loss": 0.4512,
      "step": 190
    },
    {
      "epoch": 0.16069097117605705,
      "grad_norm": 0.16737189888954163,
      "learning_rate": 0.00018955823293172693,
      "loss": 0.4082,
      "step": 200
    },
    {
      "epoch": 0.16872551973485989,
      "grad_norm": 0.14347544312477112,
      "learning_rate": 0.0001890227576974565,
      "loss": 0.4093,
      "step": 210
    },
    {
      "epoch": 0.17676006829366275,
      "grad_norm": 0.1646565943956375,
      "learning_rate": 0.00018848728246318607,
      "loss": 0.437,
      "step": 220
    },
    {
      "epoch": 0.1847946168524656,
      "grad_norm": 0.15071415901184082,
      "learning_rate": 0.00018795180722891569,
      "loss": 0.3842,
      "step": 230
    },
    {
      "epoch": 0.19282916541126846,
      "grad_norm": 0.16400277614593506,
      "learning_rate": 0.00018741633199464524,
      "loss": 0.3796,
      "step": 240
    },
    {
      "epoch": 0.2008637139700713,
      "grad_norm": 0.17444612085819244,
      "learning_rate": 0.00018688085676037483,
      "loss": 0.3439,
      "step": 250
    },
    {
      "epoch": 0.20889826252887417,
      "grad_norm": 0.23109838366508484,
      "learning_rate": 0.00018634538152610444,
      "loss": 0.3512,
      "step": 260
    },
    {
      "epoch": 0.216932811087677,
      "grad_norm": 0.15906475484371185,
      "learning_rate": 0.000185809906291834,
      "loss": 0.3493,
      "step": 270
    },
    {
      "epoch": 0.22496735964647988,
      "grad_norm": 0.14465388655662537,
      "learning_rate": 0.0001852744310575636,
      "loss": 0.3436,
      "step": 280
    },
    {
      "epoch": 0.23300190820528272,
      "grad_norm": 0.1642436385154724,
      "learning_rate": 0.0001847389558232932,
      "loss": 0.3703,
      "step": 290
    },
    {
      "epoch": 0.24103645676408555,
      "grad_norm": 0.20387201011180878,
      "learning_rate": 0.00018420348058902276,
      "loss": 0.3959,
      "step": 300
    },
    {
      "epoch": 0.24907100532288842,
      "grad_norm": 0.1556016355752945,
      "learning_rate": 0.00018366800535475234,
      "loss": 0.4068,
      "step": 310
    },
    {
      "epoch": 0.25710555388169126,
      "grad_norm": 0.15081371366977692,
      "learning_rate": 0.00018313253012048193,
      "loss": 0.4,
      "step": 320
    },
    {
      "epoch": 0.26514010244049413,
      "grad_norm": 0.11856018006801605,
      "learning_rate": 0.00018259705488621152,
      "loss": 0.3442,
      "step": 330
    },
    {
      "epoch": 0.273174650999297,
      "grad_norm": 0.14013396203517914,
      "learning_rate": 0.0001820615796519411,
      "loss": 0.3942,
      "step": 340
    },
    {
      "epoch": 0.2812091995580998,
      "grad_norm": 0.14599798619747162,
      "learning_rate": 0.0001815261044176707,
      "loss": 0.3482,
      "step": 350
    },
    {
      "epoch": 0.2892437481169027,
      "grad_norm": 0.13799016177654266,
      "learning_rate": 0.00018099062918340027,
      "loss": 0.4152,
      "step": 360
    },
    {
      "epoch": 0.29727829667570554,
      "grad_norm": 0.1324983835220337,
      "learning_rate": 0.00018045515394912986,
      "loss": 0.3439,
      "step": 370
    },
    {
      "epoch": 0.3053128452345084,
      "grad_norm": 0.140101358294487,
      "learning_rate": 0.00017991967871485944,
      "loss": 0.3737,
      "step": 380
    },
    {
      "epoch": 0.3133473937933112,
      "grad_norm": 0.11805727332830429,
      "learning_rate": 0.00017938420348058903,
      "loss": 0.3761,
      "step": 390
    },
    {
      "epoch": 0.3213819423521141,
      "grad_norm": 0.15552829205989838,
      "learning_rate": 0.00017884872824631862,
      "loss": 0.341,
      "step": 400
    },
    {
      "epoch": 0.32941649091091696,
      "grad_norm": 0.13736817240715027,
      "learning_rate": 0.0001783132530120482,
      "loss": 0.3413,
      "step": 410
    },
    {
      "epoch": 0.33745103946971977,
      "grad_norm": 0.14059296250343323,
      "learning_rate": 0.00017777777777777779,
      "loss": 0.3181,
      "step": 420
    },
    {
      "epoch": 0.34548558802852264,
      "grad_norm": 0.1314832866191864,
      "learning_rate": 0.00017724230254350737,
      "loss": 0.3851,
      "step": 430
    },
    {
      "epoch": 0.3535201365873255,
      "grad_norm": 0.12976747751235962,
      "learning_rate": 0.00017670682730923696,
      "loss": 0.3479,
      "step": 440
    },
    {
      "epoch": 0.3615546851461284,
      "grad_norm": 0.1509714275598526,
      "learning_rate": 0.00017617135207496654,
      "loss": 0.3932,
      "step": 450
    },
    {
      "epoch": 0.3695892337049312,
      "grad_norm": 0.17412757873535156,
      "learning_rate": 0.00017563587684069613,
      "loss": 0.3521,
      "step": 460
    },
    {
      "epoch": 0.37762378226373405,
      "grad_norm": 0.14991441369056702,
      "learning_rate": 0.00017510040160642571,
      "loss": 0.3286,
      "step": 470
    },
    {
      "epoch": 0.3856583308225369,
      "grad_norm": 0.13779541850090027,
      "learning_rate": 0.0001745649263721553,
      "loss": 0.3671,
      "step": 480
    },
    {
      "epoch": 0.3936928793813398,
      "grad_norm": 0.14195816218852997,
      "learning_rate": 0.00017402945113788489,
      "loss": 0.4027,
      "step": 490
    },
    {
      "epoch": 0.4017274279401426,
      "grad_norm": 0.12471170723438263,
      "learning_rate": 0.00017349397590361447,
      "loss": 0.4343,
      "step": 500
    },
    {
      "epoch": 0.40976197649894547,
      "grad_norm": 0.15163148939609528,
      "learning_rate": 0.00017295850066934406,
      "loss": 0.3442,
      "step": 510
    },
    {
      "epoch": 0.41779652505774834,
      "grad_norm": 0.15107619762420654,
      "learning_rate": 0.00017242302543507362,
      "loss": 0.3647,
      "step": 520
    },
    {
      "epoch": 0.42583107361655115,
      "grad_norm": 0.13999304175376892,
      "learning_rate": 0.00017188755020080323,
      "loss": 0.3628,
      "step": 530
    },
    {
      "epoch": 0.433865622175354,
      "grad_norm": 0.1650177240371704,
      "learning_rate": 0.00017135207496653281,
      "loss": 0.3834,
      "step": 540
    },
    {
      "epoch": 0.4419001707341569,
      "grad_norm": 0.126255601644516,
      "learning_rate": 0.00017081659973226237,
      "loss": 0.4465,
      "step": 550
    },
    {
      "epoch": 0.44993471929295975,
      "grad_norm": 0.15670904517173767,
      "learning_rate": 0.00017028112449799199,
      "loss": 0.3325,
      "step": 560
    },
    {
      "epoch": 0.45796926785176256,
      "grad_norm": 0.12534666061401367,
      "learning_rate": 0.00016974564926372157,
      "loss": 0.3643,
      "step": 570
    },
    {
      "epoch": 0.46600381641056543,
      "grad_norm": 0.1701141893863678,
      "learning_rate": 0.00016921017402945113,
      "loss": 0.3708,
      "step": 580
    },
    {
      "epoch": 0.4740383649693683,
      "grad_norm": 0.12662281095981598,
      "learning_rate": 0.00016867469879518074,
      "loss": 0.3711,
      "step": 590
    },
    {
      "epoch": 0.4820729135281711,
      "grad_norm": 0.18864040076732635,
      "learning_rate": 0.00016813922356091033,
      "loss": 0.3458,
      "step": 600
    },
    {
      "epoch": 0.490107462086974,
      "grad_norm": 0.15426218509674072,
      "learning_rate": 0.00016760374832663989,
      "loss": 0.362,
      "step": 610
    },
    {
      "epoch": 0.49814201064577684,
      "grad_norm": 0.13543865084648132,
      "learning_rate": 0.00016706827309236947,
      "loss": 0.3871,
      "step": 620
    },
    {
      "epoch": 0.5061765592045797,
      "grad_norm": 0.18093110620975494,
      "learning_rate": 0.00016653279785809908,
      "loss": 0.3232,
      "step": 630
    },
    {
      "epoch": 0.5142111077633825,
      "grad_norm": 0.15284331142902374,
      "learning_rate": 0.00016599732262382864,
      "loss": 0.4009,
      "step": 640
    },
    {
      "epoch": 0.5222456563221854,
      "grad_norm": 0.15323181450366974,
      "learning_rate": 0.00016546184738955823,
      "loss": 0.4051,
      "step": 650
    },
    {
      "epoch": 0.5302802048809883,
      "grad_norm": 0.14679692685604095,
      "learning_rate": 0.00016492637215528784,
      "loss": 0.372,
      "step": 660
    },
    {
      "epoch": 0.5383147534397911,
      "grad_norm": 0.15374250710010529,
      "learning_rate": 0.0001643908969210174,
      "loss": 0.354,
      "step": 670
    },
    {
      "epoch": 0.546349301998594,
      "grad_norm": 0.13475187122821808,
      "learning_rate": 0.00016385542168674699,
      "loss": 0.3547,
      "step": 680
    },
    {
      "epoch": 0.5543838505573968,
      "grad_norm": 0.13448861241340637,
      "learning_rate": 0.0001633199464524766,
      "loss": 0.3163,
      "step": 690
    },
    {
      "epoch": 0.5624183991161996,
      "grad_norm": 0.1288829743862152,
      "learning_rate": 0.00016278447121820616,
      "loss": 0.3632,
      "step": 700
    },
    {
      "epoch": 0.5704529476750025,
      "grad_norm": 0.11543543636798859,
      "learning_rate": 0.00016224899598393574,
      "loss": 0.3652,
      "step": 710
    },
    {
      "epoch": 0.5784874962338054,
      "grad_norm": 0.12662579119205475,
      "learning_rate": 0.00016171352074966533,
      "loss": 0.3807,
      "step": 720
    },
    {
      "epoch": 0.5865220447926082,
      "grad_norm": 0.14323389530181885,
      "learning_rate": 0.00016117804551539491,
      "loss": 0.4001,
      "step": 730
    },
    {
      "epoch": 0.5945565933514111,
      "grad_norm": 0.13263379037380219,
      "learning_rate": 0.0001606425702811245,
      "loss": 0.3554,
      "step": 740
    },
    {
      "epoch": 0.6025911419102139,
      "grad_norm": 0.14238335192203522,
      "learning_rate": 0.00016010709504685409,
      "loss": 0.3226,
      "step": 750
    },
    {
      "epoch": 0.6106256904690168,
      "grad_norm": 0.13664333522319794,
      "learning_rate": 0.00015957161981258367,
      "loss": 0.3663,
      "step": 760
    },
    {
      "epoch": 0.6186602390278196,
      "grad_norm": 0.13073422014713287,
      "learning_rate": 0.00015903614457831326,
      "loss": 0.3638,
      "step": 770
    },
    {
      "epoch": 0.6266947875866224,
      "grad_norm": 0.1339569240808487,
      "learning_rate": 0.00015850066934404284,
      "loss": 0.2989,
      "step": 780
    },
    {
      "epoch": 0.6347293361454254,
      "grad_norm": 0.1554918736219406,
      "learning_rate": 0.00015796519410977243,
      "loss": 0.4057,
      "step": 790
    },
    {
      "epoch": 0.6427638847042282,
      "grad_norm": 0.13950280845165253,
      "learning_rate": 0.000157429718875502,
      "loss": 0.3319,
      "step": 800
    },
    {
      "epoch": 0.650798433263031,
      "grad_norm": 0.11782950162887573,
      "learning_rate": 0.0001568942436412316,
      "loss": 0.3177,
      "step": 810
    },
    {
      "epoch": 0.6588329818218339,
      "grad_norm": 0.12576988339424133,
      "learning_rate": 0.00015635876840696118,
      "loss": 0.3186,
      "step": 820
    },
    {
      "epoch": 0.6668675303806367,
      "grad_norm": 0.14569640159606934,
      "learning_rate": 0.00015582329317269077,
      "loss": 0.385,
      "step": 830
    },
    {
      "epoch": 0.6749020789394395,
      "grad_norm": 0.1621841937303543,
      "learning_rate": 0.00015528781793842036,
      "loss": 0.4089,
      "step": 840
    },
    {
      "epoch": 0.6829366274982425,
      "grad_norm": 0.13134099543094635,
      "learning_rate": 0.00015475234270414994,
      "loss": 0.3466,
      "step": 850
    },
    {
      "epoch": 0.6909711760570453,
      "grad_norm": 0.11348912119865417,
      "learning_rate": 0.00015421686746987953,
      "loss": 0.3043,
      "step": 860
    },
    {
      "epoch": 0.6990057246158482,
      "grad_norm": 0.19597801566123962,
      "learning_rate": 0.0001536813922356091,
      "loss": 0.3887,
      "step": 870
    },
    {
      "epoch": 0.707040273174651,
      "grad_norm": 0.1569252610206604,
      "learning_rate": 0.0001531459170013387,
      "loss": 0.3651,
      "step": 880
    },
    {
      "epoch": 0.7150748217334538,
      "grad_norm": 0.12994404137134552,
      "learning_rate": 0.00015261044176706828,
      "loss": 0.3334,
      "step": 890
    },
    {
      "epoch": 0.7231093702922567,
      "grad_norm": 0.13774006068706512,
      "learning_rate": 0.00015207496653279787,
      "loss": 0.3561,
      "step": 900
    },
    {
      "epoch": 0.7311439188510596,
      "grad_norm": 0.14080780744552612,
      "learning_rate": 0.00015153949129852746,
      "loss": 0.331,
      "step": 910
    },
    {
      "epoch": 0.7391784674098624,
      "grad_norm": 0.11512520909309387,
      "learning_rate": 0.00015100401606425701,
      "loss": 0.3258,
      "step": 920
    },
    {
      "epoch": 0.7472130159686653,
      "grad_norm": 0.13046146929264069,
      "learning_rate": 0.00015046854082998663,
      "loss": 0.3597,
      "step": 930
    },
    {
      "epoch": 0.7552475645274681,
      "grad_norm": 0.14750921726226807,
      "learning_rate": 0.0001499330655957162,
      "loss": 0.3491,
      "step": 940
    },
    {
      "epoch": 0.7632821130862709,
      "grad_norm": 0.13117294013500214,
      "learning_rate": 0.00014939759036144577,
      "loss": 0.37,
      "step": 950
    },
    {
      "epoch": 0.7713166616450738,
      "grad_norm": 0.12348867952823639,
      "learning_rate": 0.00014886211512717538,
      "loss": 0.3567,
      "step": 960
    },
    {
      "epoch": 0.7793512102038767,
      "grad_norm": 0.11180102080106735,
      "learning_rate": 0.00014832663989290497,
      "loss": 0.3198,
      "step": 970
    },
    {
      "epoch": 0.7873857587626796,
      "grad_norm": 0.14421498775482178,
      "learning_rate": 0.00014779116465863453,
      "loss": 0.3545,
      "step": 980
    },
    {
      "epoch": 0.7954203073214824,
      "grad_norm": 0.1733960658311844,
      "learning_rate": 0.00014725568942436414,
      "loss": 0.4086,
      "step": 990
    },
    {
      "epoch": 0.8034548558802852,
      "grad_norm": 0.1372808814048767,
      "learning_rate": 0.00014672021419009373,
      "loss": 0.2998,
      "step": 1000
    },
    {
      "epoch": 0.8114894044390881,
      "grad_norm": 0.1272394061088562,
      "learning_rate": 0.00014618473895582328,
      "loss": 0.3507,
      "step": 1010
    },
    {
      "epoch": 0.8195239529978909,
      "grad_norm": 0.162174254655838,
      "learning_rate": 0.00014564926372155287,
      "loss": 0.3738,
      "step": 1020
    },
    {
      "epoch": 0.8275585015566937,
      "grad_norm": 0.16035638749599457,
      "learning_rate": 0.00014511378848728248,
      "loss": 0.3921,
      "step": 1030
    },
    {
      "epoch": 0.8355930501154967,
      "grad_norm": 0.1867516189813614,
      "learning_rate": 0.00014457831325301204,
      "loss": 0.3871,
      "step": 1040
    },
    {
      "epoch": 0.8436275986742995,
      "grad_norm": 0.13369876146316528,
      "learning_rate": 0.00014404283801874163,
      "loss": 0.3425,
      "step": 1050
    },
    {
      "epoch": 0.8516621472331023,
      "grad_norm": 0.12167710065841675,
      "learning_rate": 0.00014350736278447124,
      "loss": 0.3009,
      "step": 1060
    },
    {
      "epoch": 0.8596966957919052,
      "grad_norm": 0.12410513311624527,
      "learning_rate": 0.0001429718875502008,
      "loss": 0.3363,
      "step": 1070
    },
    {
      "epoch": 0.867731244350708,
      "grad_norm": 0.09841236472129822,
      "learning_rate": 0.00014243641231593038,
      "loss": 0.3682,
      "step": 1080
    },
    {
      "epoch": 0.8757657929095108,
      "grad_norm": 0.14756140112876892,
      "learning_rate": 0.00014190093708166,
      "loss": 0.374,
      "step": 1090
    },
    {
      "epoch": 0.8838003414683138,
      "grad_norm": 0.15321993827819824,
      "learning_rate": 0.00014136546184738956,
      "loss": 0.374,
      "step": 1100
    },
    {
      "epoch": 0.8918348900271166,
      "grad_norm": 0.17200085520744324,
      "learning_rate": 0.00014082998661311914,
      "loss": 0.3733,
      "step": 1110
    },
    {
      "epoch": 0.8998694385859195,
      "grad_norm": 0.156183660030365,
      "learning_rate": 0.00014029451137884873,
      "loss": 0.3476,
      "step": 1120
    },
    {
      "epoch": 0.9079039871447223,
      "grad_norm": 0.1381406933069229,
      "learning_rate": 0.00013975903614457834,
      "loss": 0.3429,
      "step": 1130
    },
    {
      "epoch": 0.9159385357035251,
      "grad_norm": 0.15278637409210205,
      "learning_rate": 0.0001392235609103079,
      "loss": 0.3226,
      "step": 1140
    },
    {
      "epoch": 0.923973084262328,
      "grad_norm": 0.132486492395401,
      "learning_rate": 0.00013868808567603748,
      "loss": 0.3215,
      "step": 1150
    },
    {
      "epoch": 0.9320076328211309,
      "grad_norm": 0.13444645702838898,
      "learning_rate": 0.0001381526104417671,
      "loss": 0.3593,
      "step": 1160
    },
    {
      "epoch": 0.9400421813799337,
      "grad_norm": 0.13581356406211853,
      "learning_rate": 0.00013761713520749665,
      "loss": 0.3721,
      "step": 1170
    },
    {
      "epoch": 0.9480767299387366,
      "grad_norm": 0.15123571455478668,
      "learning_rate": 0.00013708165997322624,
      "loss": 0.3785,
      "step": 1180
    },
    {
      "epoch": 0.9561112784975394,
      "grad_norm": 0.12052050977945328,
      "learning_rate": 0.00013654618473895585,
      "loss": 0.3309,
      "step": 1190
    },
    {
      "epoch": 0.9641458270563422,
      "grad_norm": 0.1255498230457306,
      "learning_rate": 0.0001360107095046854,
      "loss": 0.3858,
      "step": 1200
    },
    {
      "epoch": 0.9721803756151451,
      "grad_norm": 0.12577643990516663,
      "learning_rate": 0.000135475234270415,
      "loss": 0.3335,
      "step": 1210
    },
    {
      "epoch": 0.980214924173948,
      "grad_norm": 0.1964809000492096,
      "learning_rate": 0.00013493975903614458,
      "loss": 0.378,
      "step": 1220
    },
    {
      "epoch": 0.9882494727327509,
      "grad_norm": 0.14193300902843475,
      "learning_rate": 0.00013440428380187417,
      "loss": 0.3159,
      "step": 1230
    },
    {
      "epoch": 0.9962840212915537,
      "grad_norm": 0.12691381573677063,
      "learning_rate": 0.00013386880856760375,
      "loss": 0.3275,
      "step": 1240
    }
  ],
  "logging_steps": 10,
  "max_steps": 3735,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.650021986946253e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
